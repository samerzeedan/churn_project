# churn_project
End-to-end Machine Learning &amp; Deep Learning project for predicting customer churn using Telco dataset.
# ğŸ“Š Customer Churn Prediction â€” Machine Learning & Deep Learning Project

This project presents a complete **end-to-end churn prediction system** built using both classical Machine Learning models and a Deep Learning (MLP) model. The solution is applied to the Telco Customer Churn dataset and aims to accurately identify customers who are at risk of leaving.

The project includes data preprocessing, feature engineering, model training, hyperparameter optimization, experiment tracking, and deployment-ready prediction scripts.

---

## ğŸš€ Project Highlights

### âœ”ï¸ 1. Complete Preprocessing Pipeline
A full, production-ready preprocessing workflow including:

- **Handling Missing Values**
  - SimpleImputer (mean, median, most_frequent)
  - Advanced Imputation (KNN Imputer, Iterative Imputer)
- **Encoding categorical variables**
- **Scaling with RobustScaler** to handle outliers
- **Outlier detection and treatment**
- **Feature Engineering**
  - `TotalServicesCount`
  - `AutomaticPayment`
  - `IsNewCustomer`
  - `TenureServicesScore`
  - `InternetType` mapping  
  - Interaction features between tenure and services
- Dataset balancing using **SMOTE**

---

### âœ”ï¸ 2. Machine Learning Models Implemented

| Model | Description |
|-------|-------------|
| Logistic Regression | Baseline linear model |
| Random Forest | High-performing ensemble method |
| KNN | Distance-based algorithm |
| SVM | Non-linear classifier with kernels |
| Decision Tree | Simple interpretable model |
| Gradient Boosting | Sequential boosting algorithm |
| XGBoost | Optimized boosting algorithm |

All models were evaluated using:
- **Stratified 5-Fold Cross Validation**
- **AUC**, **Accuracy**, **Log Loss**, **Confusion Matrix**

---

### âœ”ï¸ 3. Deep Learning Model (MLP Neural Network)
A fully-connected neural network built with TensorFlow/Keras featuring:

- Multiple Dense layers  
- ReLU activation functions  
- Dropout layers  
- Batch Normalization  
- Adam optimizer  
- Early stopping  

This model achieved the **highest AUC and accuracy** across all experimentsâ€”making it the final selected model.

---

### âœ”ï¸ 4. Hyperparameter Tuning with Optuna

Optuna was used to automatically search for the **optimal hyperparameters** for all ML models:

- Logistic Regression (C)
- Random Forest (depth, estimators, min samples)
- KNN (n_neighbors, weights)
- SVM (C, gamma)
- Decision Tree (max_depth, criterion)
- Gradient Boosting (estimators, learning rate)
- XGBoost (depth, subsample, colsample_bytree, learning_rate)

The optimization objective was **maximize Validation AUC**.

---

### âœ”ï¸ 5. Experiment Tracking with MLflow
MLflow tracked:

- Model parameters  
- Training metrics  
- Cross-validation scores  
- Loss curves  
- AUC values  
- Saved model artifacts  

This allows full reproducibility and experiment comparison.

---

### âœ”ï¸ 6. Deployment-Ready Prediction Script
The project includes a ready-to-use script:

`demo_basic.py`

It performs:
- Loading scaler  
- Loading feature names  
- Loading ML or DL final model  
- Converting input into correct format  
- Predicting churn probability  

---

## ğŸ“ Project Structure

churn_project/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ demo_basic.py
â”‚â”€â”€ mlflow.db (optional)
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚ â”œâ”€â”€ processed_churn.csv
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ final_xgb_pipeline.pkl
â”‚ â”œâ”€â”€ feature_names.pkl
â”‚ â”œâ”€â”€ scaler.pkl
â”‚ â”œâ”€â”€ best_dl_model.keras
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ preprocess.ipynb
â”‚ â”œâ”€â”€ model_ml.ipynb
â”‚ â”œâ”€â”€ model_dl.ipynb
â”‚ â”œâ”€â”€ best_deep_learning.ipynb

yaml


---

## ğŸ“ˆ Model Performance

| Model | AUC | Accuracy |
|-------|------|-----------|
| Logistic Regression | ... | ... |
| Random Forest | ... | ... |
| Gradient Boosting | ... | ... |
| XGBoost | ... | ... |
| **Deep Learning (MLP)** | **Highest** | **Highest** |

*(Add your final numbers here)*

---

## ğŸ§° Technologies Used

- Python  
- Pandas  
- NumPy  
- Scikit-learn  
- TensorFlow / Keras  
- Optuna  
- MLflow  
- Imbalanced-Learn (SMOTE)  
- Matplotlib / Seaborn  

---

## â–¶ï¸ How to Run

### Install dependencies:
pip install -r requirements.txt

shell

### Run prediction demo:
python demo_basic.py

yaml


---

## ğŸ“¬ Contact

If you'd like to connect or discuss the project:

- **LinkedIn:** (
www.linkedin.com/in/samer-zaidan-60bb372b0

)

---
